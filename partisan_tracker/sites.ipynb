{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "import nltk\r\n",
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n",
                "import re\r\n",
                "import requests\r\n",
                "import sqlite3 as sql\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "import networkx as ntx \r\n",
                "import time \r\n",
                "from urllib.parse import urlparse\r\n",
                "from bs4 import BeautifulSoup\r\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
                "from config_new import config\r\n",
                "import pytrends\r\n",
                "import json\r\n",
                "\r\n",
                "from nlp_helper import *\r\n",
                "base_urls = {'NRSC':'https://www.nrsc.org/press-releases/',\r\n",
                "'DSCC':'https://www.dscc.org/news/',\r\n",
                "'DCCC':'https://dccc.org/press-release/',\r\n",
                "'NRCC':'https://www.nrcc.org/press-room/',\r\n",
                "'RNC' :'https://www.gop.com/press-release/',\r\n",
                "'DNC':'https://democrats.org/news/'}"
            ],
            "metadata": {
                "azdata_cell_guid": "dd2e02fd-5f98-4dcc-ac1e-c7af86636e8d",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "code",
            "source": [
                "# con = sql.connect(config['database'])\r\n",
                "# cur = con.cursor()\r\n",
                "# cur.execute(\"\"\" DROP TABLE IF EXISTS vocab_urls; \"\"\")\r\n",
                "# cur.execute(\"\"\"DROP TABLE IF EXISTS urls_text; \"\"\")\r\n",
                "# cur.execute(\"\"\"DROP TABLE IF EXISTS static_text; \"\"\")\r\n",
                "# cur.execute(\"\"\"DROP TABLE IF EXISTS text_vader_cos_similarity; \"\"\")\r\n",
                "# cur.execute(\"\"\"DROP TABLE IF EXISTS title_vader_cos_similarity; \"\"\")\r\n",
                "# cur.execute(\"\"\"DROP TABLE IF EXISTS base_url; \"\"\")\r\n",
                "# cur.execute(\"\"\"DROP TABLE IF EXISTS catagory; \"\"\")               \r\n",
                "# cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS catagory (id INTEGER PRIMARY KEY AUTOINCREMENT, catagory TEXT)\r\n",
                "# \"\"\")\r\n",
                "# cur.execute(\"\"\"\r\n",
                "#     CREATE TABLE IF NOT EXISTS base_url (id INTEGER PRIMARY KEY AUTOINCREMENT, catagory_id_array TEXT, scheme TEXT, netloc TEXT, path TEXT, slug TEXT )\r\n",
                "# \"\"\")\r\n",
                "# cur.execute(\"\"\"\r\n",
                "#     CREATE TABLE IF NOT EXISTS vocab_urls (id INTEGER PRIMARY KEY AUTOINCREMENT, base_url_id INTEGER, path TEXT)\r\n",
                "# \"\"\")\r\n",
                "# cur.execute(\"\"\"\r\n",
                "#     CREATE TABLE IF NOT EXISTS urls_text (id INTEGER PRIMARY KEY AUTOINCREMENT, vocab_url_id INTEGER, p_tag_list TEXT, a_tag_list BLOB, title_tag_text TEXT, pull_dms TEXT)\r\n",
                "# \"\"\")\r\n",
                "# cur.execute(\"\"\"\r\n",
                "#     CREATE TABLE IF NOT EXISTS static_text (id INTEGER PRIMARY KEY AUTOINCREMENT, base_url_id INTEGER,  text_block TEXT, type TEXT)\r\n",
                "# \"\"\")\r\n",
                "# cur.execute(\"\"\"\r\n",
                "#     CREATE TABLE IF NOT EXISTS text_vader_cos_similarity (id INTEGER PRIMARY KEY AUTOINCREMENT, urls_text_id INTEGER,bag_a_words_json BLOB, Tfidf_json BLOB, sediment_neg REAL, sediment_neu REAL, sediment_pos REAL, sediment_comp REAL, week_average_similarity REAL )\r\n",
                "# \"\"\")\r\n",
                "# cur.execute(\"\"\"\r\n",
                "#     CREATE TABLE IF NOT EXISTS title_vader_cos_similarity (id INTEGER PRIMARY KEY AUTOINCREMENT, urls_text_id INTEGER, sediment_neg REAL, sediment_neu REAL, sediment_pos REAL, sediment_comp REAL, post_vocab_similarity REAL )\r\n",
                "# \"\"\")\r\n",
                "# cur.execute(\"\"\"CREATE VIEW IF NOT EXISTS Report \r\n",
                "#                 AS \r\n",
                "#                 SELECT slug, a.netloc || CASE WHEN a.path = '/' THEN '' ELSE a.path END || b.path AS 'url',\r\n",
                "#                     c.p_tag_list,\r\n",
                "#                     c.title_tag_text,\r\n",
                "#                     d.sediment_comp AS title_comp,\r\n",
                "# \t\t\t\t\td.post_vocab_similarity AS title_comp_post,\r\n",
                "#                     e.bag_a_words_json,\r\n",
                "# \t\t\t\t\te.Tfidf_json,\r\n",
                "# \t\t\t\t\te.sediment_comp AS post_comp\r\n",
                "#                 FROM base_url a\r\n",
                "#                 JOIN vocab_urls b ON a.id = b.base_url_id\r\n",
                "#                 JOIN urls_text c ON b.id = c.vocab_url_id\r\n",
                "#                 JOIN title_vader_cos_similarity d ON c.id = d.urls_text_id\r\n",
                "# \t\t\t\tJOIN text_vader_cos_similarity e ON c.id = e.urls_text_id\"\"\")\r\n",
                "# cur.execute(f\"INSERT INTO catagory (catagory) VALUES ('Republican'),('Democrate'),('3rd Party'),('Natioanl'),('Local'),('Party'),('Independent'),('News')\")\r\n",
                "# for key, value in base_urls.items():\r\n",
                "#     u = urlparse(value)\r\n",
                "#     cur.execute(f\"INSERT INTO base_url (slug, scheme, netloc, path) VALUES ('{key}','{u.scheme}','{u.netloc}','{u.path}')\")\r\n",
                "# con.commit()\r\n",
                "# con.close()"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "5281b1da-cd84-4052-a7fc-8663c07b8415",
                "tags": []
            },
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "def removeValue(givenlist, value):\r\n",
                "    while value in givenlist:\r\n",
                "        givenlist.remove(value)\r\n",
                "    return givenlist\r\n",
                "\r\n",
                "def link_gather(base_urls_idx,con):\r\n",
                "    cur = con.cursor()\r\n",
                "    base_urls = pd.read_sql_query(f\"SELECT id, a.scheme || '://' || a.netloc || a.path as baseURL from base_url a where id = {base_urls_idx}\", con)\r\n",
                "    base_urls = base_urls['baseURL'].to_list()[0]\r\n",
                "    x = requests.get(base_urls, headers={'User-Agent': 'Mozilla/5.0'})\r\n",
                "    soup = BeautifulSoup(x.text, 'html.parser')\r\n",
                "    el = soup.body.find_all('a', href=True)\r\n",
                "    urls = [i['href'] for i in el]\r\n",
                "    post = []\r\n",
                "    for i in urls:\r\n",
                "        if (urlparse(base_urls).path != urlparse(i).path and urlparse(base_urls).netloc == urlparse(i).netloc) or urlparse('/news/breaking-news/').netloc == '':\r\n",
                "            post.append(i)\r\n",
                "    post = list(set(post))\r\n",
                "    df_post = pd.read_sql_query(f\"\"\"\r\n",
                "        SELECT b.scheme || '://' || b.netloc || a.path AS url_path FROM vocab_urls a JOIN base_url b ON b.id = a.base_url_id WHERE base_url_id = {base_urls_idx}\r\n",
                "    \"\"\", con)\r\n",
                "    scrapped = df_post['url_path'].to_list()\r\n",
                "    for link in scrapped:\r\n",
                "        post = removeValue(post,link)\r\n",
                "    for link in post:\r\n",
                "        u = urlparse(link)\r\n",
                "        cur.execute(f\"INSERT INTO vocab_urls (base_url_id, path) VALUES ({base_urls_idx}, '{u.path}')\")\r\n",
                "        con.commit()\r\n",
                "    return post\r\n",
                "\r\n",
                "def static(current,previous):\r\n",
                "    static_text = []\r\n",
                "    for i in current:\r\n",
                "        for j in previous:\r\n",
                "            try:\r\n",
                "                score = np.round(np.sum(cos_similarity([i,j])[0]))\r\n",
                "                if score == 2 :\r\n",
                "                    static_text.append(i)\r\n",
                "            except:\r\n",
                "                static_text.append(i)\r\n",
                "    static_text = list(set(static_text))\r\n",
                "    return static_text\r\n",
                "\r\n",
                "def blog_post_scrap(vurl_idx,url,con):\r\n",
                "    cur = con.cursor()\r\n",
                "    diff_clean = {'current':{'title':None,'links':None,'text':None}, 'previous':{'title':None,'links':None,'text':None}}\r\n",
                "    try:\r\n",
                "        link = url\r\n",
                "        vurl_idx = vurl_idx\r\n",
                "        x = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\r\n",
                "        soup = BeautifulSoup(x.text, 'html.parser')\r\n",
                "        p = soup.body.find_all('p')\r\n",
                "        a = soup.body.find_all('a', href=True)\r\n",
                "        title = soup.find('title').text\r\n",
                "        links = [i['href'] for i in a]\r\n",
                "        text = []\r\n",
                "        for i in p:\r\n",
                "            try:\r\n",
                "                j = i.text.replace(\" \", \"\").replace('\\n',' ')\r\n",
                "                if len(j)>0:\r\n",
                "                    text.append(i.text.replace('\\n',' '))\r\n",
                "            except:\r\n",
                "                continue\r\n",
                "        static_df = pd.read_sql_query(f\"\"\"SELECT text_block, type FROM static_text WHERE  base_url_id = {vurl_idx}\"\"\",con)\r\n",
                "        if not static_df['text_block'].to_list():\r\n",
                "            if not diff_clean['current']['text']:\r\n",
                "                diff_clean['current']['url'] = link\r\n",
                "                diff_clean['current']['idx'] = vurl_idx\r\n",
                "                diff_clean['current']['text'] = text\r\n",
                "                diff_clean['current']['links'] = links\r\n",
                "                diff_clean['current']['title'] = title\r\n",
                "            else:\r\n",
                "                diff_clean['previous']['url'] = link\r\n",
                "                diff_clean['previous']['idx'] = vurl_idx\r\n",
                "                diff_clean['previous']['text'] = text\r\n",
                "                diff_clean['previous']['links'] = links\r\n",
                "                diff_clean['previous']['title'] = title\r\n",
                "            if  diff_clean['current']['text'] and diff_clean['previous']['text']:\r\n",
                "                current_text = diff_clean['current']['text']\r\n",
                "                previous_text = diff_clean['previous']['text']\r\n",
                "                static_text = static(current_text,previous_text)\r\n",
                "                current_link = diff_clean['current']['links']\r\n",
                "                previous_link = diff_clean['previous']['links']\r\n",
                "                static_links = static(current_link,previous_link)\r\n",
                "                for s_text in static_text:\r\n",
                "                    cur.execute(f\"\"\"INSERT INTO static_text (base_url_id, text_block, type) VALUES ({vurl_idx} ,'{s_text.replace(\"'\",\"''\").replace('\"','\"\"')}', 'text') \"\"\")\r\n",
                "                    con.commit()\r\n",
                "                for a_text in static_links:\r\n",
                "                    cur.execute(f\"\"\"INSERT INTO static_text (base_url_id, text_block, type) VALUES ({vurl_idx} ,'{a_text}', 'link') \"\"\")\r\n",
                "                    con.commit()\r\n",
                "                for key in diff_clean.keys():\r\n",
                "                    blog_post_scrap(diff_clean[key]['idx'],diff_clean[key]['url'],con)\r\n",
                "        else:\r\n",
                "            for idx, row in static_df.iterrows():\r\n",
                "                if row['type'] == 'text':\r\n",
                "                    text = removeValue(text, row['text_block'])\r\n",
                "                elif row['type'] == 'link':\r\n",
                "                    links = removeValue(links, row['text_block'])\r\n",
                "            text = ' '.join(text)\r\n",
                "            text = text.replace(\"'\",\"''\")\r\n",
                "            links = ','.join(links)\r\n",
                "            cur.execute(f\"\"\"INSERT INTO urls_text (vocab_url_id , p_tag_list , a_tag_list , title_tag_text , pull_dms) VALUES ({vurl_idx},'{text.replace(\"'\",\"''\")}','{links}','{title.replace(\"'\",\"''\")}','{time.strftime('%Y-%m-%d', time.localtime())}')\"\"\")\r\n",
                "            con.commit()\r\n",
                "    except Exception as e:\r\n",
                "        print(e)"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "6fb8d4e1-9fe0-42cf-8359-36b8620b3ba0"
            },
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "## Dummy Inputs\r\n",
                "con = sql.connect(config['database'])\r\n",
                "links = link_gather(idx_dummy,con)\r\n",
                "cur = con.cursor()\r\n",
                "post_data = pd.read_sql_query(f\"\"\"SELECT a.id, b.scheme || '://' || b.netloc || a.path AS url FROM vocab_urls a JOIN base_url b ON b.id = a.base_url_id WHERE a.base_url_id = {idx_dummy} and a.id NOT IN (SELECT vocab_url_id FROM urls_text)\"\"\", con)\r\n",
                "# Function Blog Posts\r\n",
                "for idx, row in post_data.iterrows():\r\n",
                "    blog_post_scrap(row['id'],row['url'],con)\r\n",
                "con.close()\r\n",
                "# # Text NLP\r\n",
                "con = sql.connect(config['database'])\r\n",
                "id_list = pd.read_sql_query(\"SELECT id FROM urls_text WHERE id NOT IN (SELECT urls_text_id FROM title_vader_cos_similarity) \",con)\r\n",
                "id_list = id_list['id'].tolist()\r\n",
                "for idnum in id_list:\r\n",
                "    try:\r\n",
                "        row = pd.read_sql_query(f\"SELECT * FROM urls_text WHERE id = {idnum} \",con)\r\n",
                "        idx = row['id'].values[0]\r\n",
                "\r\n",
                "        title_tag_text = row['title_tag_text'].values[0]\r\n",
                "        title_tag_block = processed_feature(title_tag_text)\r\n",
                "        title_vadar = vadar(title_tag_block)\r\n",
                "\r\n",
                "        p_tag_list = re.sub(r'[\\d.]*\\d+', '',row['p_tag_list'].values[0])\r\n",
                "        p_tag_block = processed_feature(p_tag_list)\r\n",
                "        p_vocab = create_bag_of_words(p_tag_block)\r\n",
                "        p_vadar = vadar(p_tag_block)\r\n",
                "        p_Tfidf = Tfidf(p_tag_list.split('.'))\r\n",
                "        dict_Tfidf = {key:value for key, value in p_Tfidf}\r\n",
                "        dict_Tfidf = dict(sorted(dict_Tfidf.items(),key = lambda x:x[1], reverse = True))\r\n",
                "        avg = sum(dict_Tfidf.values())/len(dict_Tfidf.keys())\r\n",
                "        dict_Tfidf_insert = {}\r\n",
                "        for key, value in dict_Tfidf.items():\r\n",
                "            if value > avg:\r\n",
                "                dict_Tfidf_insert[key] =value \r\n",
                "        \r\n",
                "        dict_vocab_insert = dict(nltk.FreqDist(p_vocab).most_common(len(dict_Tfidf_insert.keys())))\r\n",
                "        tvp = cos_similarity([title_tag_block,' '.join(list(dict_vocab_insert.keys()))])[0][1]\r\n",
                "\r\n",
                "        cur.execute(f\"\"\"INSERT INTO title_vader_cos_similarity (urls_text_id, sediment_neg, sediment_neu, sediment_pos, sediment_comp, post_vocab_similarity ) VALUES ({idnum}, {title_vadar['neg']}, {title_vadar['neu']}, {title_vadar['pos']}, {title_vadar['comp']}, {tvp});\"\"\")\r\n",
                "        cur.execute(f\"\"\"INSERT INTO text_vader_cos_similarity (urls_text_id, bag_a_words_json, Tfidf_json, sediment_neg, sediment_neu, sediment_pos, sediment_comp) VALUES ({idnum}, '{json.dumps(dict_vocab_insert)}', '{json.dumps(dict_Tfidf)}', {p_vadar['neg']}, {p_vadar['neu']}, {p_vadar['pos']}, {p_vadar['comp']});\"\"\")\r\n",
                "        con.commit()\r\n",
                "    except Exception as e:\r\n",
                "        continue\r\n",
                "con.close()"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "f160258c-0ce9-4311-aa8f-7fc4e1346f24",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Failed to parse: https://bonnercountydailybee.comtel:2082639534\n"
                }
            ],
            "execution_count": 4
        }
    ]
}