{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "import datetime\r\n",
                "import sqlite3 as sql \r\n",
                "import requests\r\n",
                "import re\r\n",
                "import nltk\r\n",
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
                "import json \r\n",
                "import networkx\r\n",
                "import os\r\n",
                "from nlp_helper import *\r\n",
                "from pdfminer.high_level import extract_text\r\n",
                "from config_new import config\r\n",
                "nltk.download('punkt')\r\n",
                "nltk.download('stopwords')\r\n",
                "nltk.download('averaged_perceptron_tagger')\r\n",
                "nltk.download('vader_lexicon')\r\n",
                "nltk.download('wordnet')\r\n",
                "nltk.download('omw-1.4')"
            ],
            "metadata": {
                "azdata_cell_guid": "c6926092-1817-465e-9dd7-6518bf4aa35c",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n",
                    "output_type": "stream"
                },
                {
                    "name": "stderr",
                    "text": "[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n",
                    "output_type": "stream"
                },
                {
                    "name": "stderr",
                    "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "True"
                    },
                    "metadata": {},
                    "execution_count": 2,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "code",
            "source": [
                "con = sql.connect(config['database'])\r\n",
                "cur = con.cursor()\r\n",
                "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS Front_Page_text (id INTEGER PRIMARY KEY AUTOINCREMENT, front_page_text TEXT, date_dd_mm_yyyy TEXT, slug TEXT)\"\"\")\r\n",
                "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS Front_Page_vader (front_page_text_id INTEGER, sediment_neg REAL, sediment_neu REAL, sediment_pos REAL, sediment_comp REAL)\"\"\")\r\n",
                "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS Front_Page_bag (front_page_text_id INTEGER, bag_a_words_json BLOB, Tfidf_json BLOB, phrase_list BLOB)\"\"\")\r\n",
                "cur.execute(\"\"\"CREATE VIEW IF NOT EXISTS Front_Page_Report\r\n",
                "               AS\r\n",
                "               SELECT a.slug, a.date_dd_mm_yyyy, b.sediment_comp, b.sediment_neg, b.sediment_neu, b.sediment_pos, c.bag_a_words_json, c.Tfidf_json\r\n",
                "                FROM Front_Page_text a\r\n",
                "                JOIN Front_Page_vader b ON a.id = b.front_page_text_id\r\n",
                "                JOIN Front_Page_bag c ON a.id = c.front_page_text_id\"\"\")\r\n",
                "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS Front_Page_nea (front_page_text_id INT, nea_text TEXT, start_char TEXT, end_char TEXT, label TEXT)\"\"\") \r\n",
                "con.commit()"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "11bbbad1-4976-48ae-8071-2cf003fd5332",
                "tags": []
            },
            "outputs": [],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "con = sql.connect(config['database'])\r\n",
                "day = datetime.datetime.today()\r\n",
                "cur = con.cursor()\r\n",
                "slugs = pd.read_sql_query(\"\"\"SELECT * FROM Front_Page_slug\"\"\",con)\r\n",
                "slugs = slugs['slug'].tolist()\r\n",
                "for slug in slugs:\r\n",
                "    url_temp =  f'https://cdn.freedomforum.org/dfp/pdf{day.day}/{slug}.pdf'\r\n",
                "    r = requests.get(url_temp)\r\n",
                "    if r.status_code == 200:\r\n",
                "        with open(f'{slug}.pdf', 'wb') as f:\r\n",
                "            f.write(r.content)\r\n",
                "        text = extract_text(f'{slug}.pdf')\r\n",
                "        os.remove(f'{slug}.pdf')\r\n",
                "        cur.execute(f\"\"\"INSERT INTO Front_Page_text (front_page_text, day, slug) VALUES ('{text.replace(\"'\",\"''\")}', '{day.day}-{day.month}-{day.year}', '{slug}')\"\"\")\r\n",
                "        con.commit()\r\n",
                "con.close()"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "71b161a5-9706-48c1-adbf-a32eaacae84a",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "d5f35de8-dd3c-44b6-a3d8-2a27a7e6ad5c",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "empty vocabulary; perhaps the documents only contain stop words\n \nempty vocabulary; perhaps the documents only contain stop words\n \nempty vocabulary; perhaps the documents only contain stop words\n \nempty vocabulary; perhaps the documents only contain stop words\n \n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "empty vocabulary; perhaps the documents only contain stop words\n \n"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "67d5d086-672b-48c1-a63a-93d49ab99e42"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/plain": "' '"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 8
        }
    ]
}