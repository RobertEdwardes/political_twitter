{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "import nltk\r\n",
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n",
                "import re\r\n",
                "import sqlite3 as sql\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
                "nltk.download('punkt')\r\n",
                "nltk.download('stopwords')\r\n",
                "nltk.download('averaged_perceptron_tagger')\r\n",
                "nltk.download('vader_lexicon')\r\n",
                "nltk.download('wordnet')\r\n",
                "nltk.download('omw-1.4')"
            ],
            "metadata": {
                "azdata_cell_guid": "8b8a3698-f966-4194-bcd0-54a2c8298d59",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stderr",
                    "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     C:\\Users\\vaugh\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "True"
                    },
                    "metadata": {},
                    "execution_count": 32,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 32
        },
        {
            "cell_type": "code",
            "source": [
                "# n for noun files, v for verb files, a for adjective files, r for adverb\r\n",
                "pos_to_lemmatize={'NN':'n','NNS':'n','NNP':'n','NPPS':'n','WP':'n','WP$':'n',\r\n",
                "                 'VB':'v','VBD':'v','VBG':'v','VBN':'v','VBP':'v','VBZ':'v',\r\n",
                "                 'JJ':'a','JJR':'a','JJS':'a',\r\n",
                "                 'RB':'r','RBR':'r','RBS':'r','WRB':'r'}\r\n",
                "\r\n",
                "def processed_feature(text):\r\n",
                "    # Removing URLS\r\n",
                "    processed_feature = re.sub(r'https?:\\S+', '', text)\r\n",
                "    # Remove all the special characters\r\n",
                "    processed_feature = re.sub(r'\\W', ' ', processed_feature)\r\n",
                "    # remove all single characters\r\n",
                "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\r\n",
                "    # Remove single characters from the start\r\n",
                "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \r\n",
                "    # Substituting multiple spaces with single space\r\n",
                "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\r\n",
                "    # Removing prefixed 'b'\r\n",
                "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\r\n",
                "    # Converting to Lowercase remove RT for retweets\r\n",
                "    processed_feature = processed_feature.lower().replace('RT ','')\r\n",
                "    return processed_feature\r\n",
                "\r\n",
                "def create_bag_of_words(text, lemmatize=True):\r\n",
                "    tokenized_word = nltk.tokenize.word_tokenize(text)\r\n",
                "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\r\n",
                "    filtered_sent=[]\r\n",
                "    for w in tokenized_word:\r\n",
                "        if w not in stop_words:\r\n",
                "            filtered_sent.append(w)\r\n",
                "    pos_taged = nltk.pos_tag(filtered_sent)\r\n",
                "    ps  = nltk.stem.PorterStemmer()\r\n",
                "    lem = nltk.stem.wordnet.WordNetLemmatizer()\r\n",
                "    bag_of_words = []\r\n",
                "    if lemmatize:\r\n",
                "        for tag in pos_taged:\r\n",
                "            if tag[1] in pos_to_lemmatize:\r\n",
                "                bag_of_words.append(lem.lemmatize(tag[0],pos_to_lemmatize[tag[1]]))\r\n",
                "            else:\r\n",
                "                bag_of_words.append(ps.stem(tag[0]))\r\n",
                "    else:\r\n",
                "        for w in filtered_sent:\r\n",
                "            bag_of_words.append(ps.stem(w))\r\n",
                "    return bag_of_words\r\n",
                "\r\n",
                "def vadar(text):\r\n",
                "    out_put={'neg':None,\r\n",
                "             'neu':None,\r\n",
                "             'pos':None,\r\n",
                "             'Comp':None}\r\n",
                "    sia = SentimentIntensityAnalyzer()\r\n",
                "    out_put['neg'] = sia.polarity_scores(text)['neg']\r\n",
                "    out_put['neu'] = sia.polarity_scores(text)['neu']\r\n",
                "    out_put['pos'] = sia.polarity_scores(text)['pos']\r\n",
                "    out_put['Comp'] = sia.polarity_scores(text)['compound']\r\n",
                "    return out_put\r\n",
                "\r\n",
                "def cos_similarity(textlist):\r\n",
                "    TfidfVec = TfidfVectorizer()\r\n",
                "    tfidf = TfidfVec.fit_transform(textlist)\r\n",
                "    return (tfidf * tfidf.T).toarray()\r\n",
                "\r\n",
                "def setiment(con, inc_rt=False):\r\n",
                "    cur = con.cursor()\r\n",
                "    _proc = pd.read_sql_query(\"SELECT tweet_id FROM tweet_setiment\")\r\n",
                "    _proc = _proc['tweet_id'].tolist()\r\n",
                "    if not inc_rt:\r\n",
                "        df = pd.read_sql_query(f'SELECT * FROM tweets WHERE retweet = \"False\" AND tweet_id NOT IN ({_proc})')\r\n",
                "    else:\r\n",
                "        df = pd.read_sql_query(f'SELECT * FROM tweets WHERE tweet_id NOT IN ({_proc})')\r\n",
                "    full_pt = []\r\n",
                "    for x,y in list(set(df[['pull_time','query_idx']].tolist()))\r\n",
                "        df_chunck = df[df['pull_time'] == x & df['query_idx'] == y ]\r\n",
                "        for idx, row in df_chunck.iterrows():\r\n",
                "            pt = processed_feature(row['text']))\r\n",
                "            tw= create_bag_of_words(pt)\r\n",
                "            op = vadar(pt)\r\n",
                "            cur.execute(f\"\"\"INSERT INTO tweet_setiment (tweet_id , tokens , setiment_neg , setiment_neu , setiment_pos , setiment_comp )\r\n",
                "                            VALUES({row['tweet_id']},\"{tw}\",{op['neg']},{op['neu']},{op['pos']},{op['comp']}\"\"\")\r\n",
                "            con.commit()\r\n",
                "            full_pt.append(pt)\r\n",
                "        sim = cos_similarity(full_pt)\r\n",
                "        query_pull_mean = np.matrix(sim).mean()\r\n",
                "        bag_group = create_bag_of_words(' '.join(full_pt))\r\n",
                "        str_agg = vadar(' '.join(full_pt))\r\n",
                "        fd = nltk.probability.FreqDist(bag_group)\r\n",
                "        feq = fd.most_common(10)\r\n",
                "        cur.execute(f\"\"\"INSERT INTO query_setiment (query_idx , pull_time , tokens , setiment_neg , setiment_neu , setiment_pos , setiment_comp , average_similarity, rt_included )\r\n",
                "                        VALUES ({row['query_idx']},\"{row['pull_time']}\",\"{feq}\",{str_agg['neg']},{str_agg['neu']},{str_agg['pos']},{str_agg['comp']}, {query_pull_mean}, \"{inc_rt}\")\"\"\")\r\n",
                "    con.commit()\r\n",
                "    con.close()"
            ],
            "metadata": {
                "azdata_cell_guid": "9b7e7207-677e-4233-b785-669172bc2233",
                "tags": []
            },
            "outputs": [],
            "execution_count": 94
        }
    ]
}